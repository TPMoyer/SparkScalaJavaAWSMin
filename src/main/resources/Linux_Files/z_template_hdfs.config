# configuration (.config) file for an Oracle RDBS 
#
# a .config is expected to reflect a clients standard behaviors/choices and will thus, be changed little over time
# a .properties file, is expected to be modified a couple of times as a user works
# though the RDBS exploration process, as he/she settles on the schemas/tabes desired 
#
# if a property in a .config file contains a collen (:) it needs to be enclosed in double quotes
# properties in a .property file containing collen's need not be double quoted


# Sublime Text 3 can be set to show this file with color differentiated syntax 
# top menue bar        view...    syntax...   java...   java properties

# The .jar which will be called by a XX_caller.sh in linux or a XX_caller.cmd on windows
jarFid=./Rdbs2MetaData2HiveTables.jar
initialJVMMemoryPool=2g 

javaHomeExportRequired4App=false
javaHomeExport4App=/usr/java/jdk1.8.0_121

spark2pointXExportRequired4App=false
spark2pointXHome4App=/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2

# acceptable values are parquet or orc
rcFormat=orc


#for databaseVendor pick from among  Oracle, SQL_Server, MsAccess, Cache
databaseVendor=Oracle
databaseVersion=11.x 
# for SQL_server this is the CATALOG_NAME,  for Oracle it is the UserId
databaseName=kariba_dm
# For SQL_server this is the schema under the CATALOG_NAME.
# For Oracle it should be the uerId, same as databaseName.
databaseSchema=kariba_dm

# databaseConnectionStrings can be a real pain.   
#
# A tactic suggested is:
# First get a connection-to-your-RDBS-instance to work with an open source database connectivity tool
# such as SQuirreL SQL Client, toad, or my favorite: SQL_Developer
# and to then copy the known working connection string into this .properties file.
#
# This is the format for an Oracle connectionString with a Service Name
# databaseConnectionString=jdbc:oracle:thin:@156.30.216.134:1521/IDWQA.qdx.com
#
# This is the format for an Oracle connectionString with an SID
# databaseConnectionString=jdbc:oracle:thin:@156.30.216.134:1527:DTCPRD
#
# This is the format for a Cache connectionString
#databaseConnectionString=jdbc:Cache://C3QADB1:1973/QA
#
# This is the format for a MySQL connectionString
#databaseConnectionString=jdbc:mysql://10.200.2.30:3306/assumptions
#
# This is the format for a SQL_Server connectionString
#databaseConnectionString=jdbc:jtds:sqlserver://10.41.126.113:1433;databaseName=CTIData
#
# This is the format for an MsAccess databaseConnectionString
# Notice the windows imposed \\ where-ever a CLI has a \
# databaseConnectionString=jdbc:ucanaccess://\\\\Qdcws1072\\rsc_db$\\PatientAdvocacy\\PatientAdvocacy.mdb;memory=false
#

databaseConnectionString="jdbc:oracle:thin:@infalab.cd1wzz6z3zgd.us-east-1.rds.amazonaws.com:1521/ORCL"

# user name of the RDBS account to be used
databaseUserId=kariba_dm

# Passwords for this RDBMS account: 
# File on the executing OS which contains one line, one word, which is the unencrypted password for the database for the $databaseUserId
# RMH uses a password file on the local fileSystem
databasePasswordFidHostOS="/home/tpmoyer/kif/passwords/oracle_demo.txt"
# Sqoop (which is not called by RMH, but is called in the data loading phase) can use an unencryped password in a file on the HDFS fileSystem
# or can use the encrypted password in a HadoopCredentialProvider
databasePasswordFidHDFS=/user/tpmoyer/passwords/oracle_demo.txt

# if the Hadoop Credential Provider is not used, set the following databasePasswordStoredInHadoopCredentialProvider property to false
databasePasswordStoredInHadoopCredentialProvider=false
hadoopCredentialProviderPath="jceks://hdfs/credential_provider/client_name.bdl.jceks"
hadoopCredentialPasswordAlias=project.phase.database


# The horribly non-intuitive string needed to get a CLI beeline connection to the cluster:
# Your best bets are to ask the guy who has been on the cluster a while.
# Next best is a cluster administrator.
# If that fails, there's always RTFM 
#beelineOptions="-u \"jdbc:hive2://qdlsp0018.us.qdx.com:10000/default;principal=hive/_HOST@US.QDX.COM;saslQop=auth-conf\""
beelineOptions="-u \"jdbc:hive2://localhost:10000/default;\""
# Similarly for CLI impala conneection on your string sourcing:
# Special to impala is:  If this property is not present (or is commented out) Source2Refined will assume your cluster does not have impala
# and will not attempt to do the    "invalidate metadata"  commands required to keep impala concurrent with table updates
#impalaOptions="-V -k -i qdlsp0019.us.qdx.com:25003 --ssl "



# This is used when a client has a standard partitioning scheme:
# Not all the tables have partitions, but for those that do have partitions  
# are all partitioned the same way.. ie they all have the same partitions,
#hiveGlobalPartitionColumnName=dos_month_year



# Clients may have a standard column name for a timestamp which represents the moment a row was first created
# Some examples of this are   row_create_ts, create_timestamp, date_created, RC_TS, short for RecordCreate_TimeStamp
#columnNameForRecordCreateTimestamp=createdate

# Clients may have a standard column name for a timestamp which represents the moment a row was last updated
# Some examples of this are   row_update_ts, updat_timestamp, date_changed, date_modified, RU_TS, RowUpdate_TimeStamp
#columnNameForRecordUpdateTimestamp=lastmodified

# Clients may have a standard column name for a column has the property that for each new record,
# the value in this column is higher than the value of all previously created records.
# When doing an incremental load on a table, one can get all the new records by
# ingesting only those rows from the source DB which have values 
# greater than the maximum previously ingested value in this column in this table. 
# Such columns are often sequences or can be timestamp based.
# samples from some clients are  row_number, recordCreateTimeStamp, row_id 
#loadIterationsShouldKeyOffThisColumn=row_number


# userId under which the app will log into to hiveEdgeNode and the impalaEdgeNode
#userId=tpmoyer
#userPasswordFid=/home/tpmoyer/kif/passwords/tpmoyer.txt
#hiveEdgeNode=localhost:10000/default
#impalaEdgeNode=qdlsp0017.us.qdx.com
#getData=true

# ClientA data-lake boilerplate relating zones (landing, raw, tmp, refined) to the HDFS directory structure
# The three parts of a hive database name are underscore separated
# The first  part is lnd, raw, tmp, or rfd
# The second part is the project Name
# The third  part is the projectPhase
preambleLanding=lnd_
preambleRaw=raw_
preambleTemp=tmp_
preambleRefined=rfd_
#nameservice="hdfs://nameservice1"
nameservice=""
directoryTreeAboveLandingRawTempAndRefined=/iab/data/hive
dirForLanding=/landing
dirForRaw=/raw
dirForTemp=/tmp
dirForRefined=/refined
databaseNameAbbreviation4Landing=lnd
databaseNameAbbreviation4Raw=raw
databaseNameAbbreviation4Temp=tmp
databaseNameAbbreviation4Refined=rfd

# A short name is best here, as you will be typing it in many queries 
project=claims
#subProject=salesPortal

# select from among a short list of options
# for one client, these were    dev, qa, prod
# we chose dev, qa, prod
projectPhase=dev

# The variable which will be the landing and raw zone partition variable
iterationPartitionVariable=load_ts

# This option has been used as a key to customization within the code.
# Index of the case used to modify the curation of the data from this source.   -1 is default 
#databaseIndividualCuration=1

# Prepender to be added to each of the hiveTableNames from this source.  This must be all lower case
#hiveTableNamePrepender=sfdc_


#********************************************************************************************
# default behavior is to log the properties derived from this file.   
# Uncomment the following line if you want the log file to omit these
#suppressLoggingTheseProperties=true

# default behavior is to supperss the logging of the Environment properties
# Uncomment the following line if you want the log file to contain these
#suppressLoggingEnvironmentProperties=false

# default behavior is to supperss the logging of the System properties
# Uncomment the following line if you want the log file to contain these
#suppressLoggingSystemProperties=false
